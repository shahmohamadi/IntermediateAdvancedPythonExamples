{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForestt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SLJDYePTeO4qA57Ex5elwnIQmYaGQu_S",
      "authorship_tag": "ABX9TyMBzdqwZugqf9VJOLgYu5aG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahmohamadi/IntermediateAdvancedPythonExamples/blob/master/RandomForestt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdDhQHtsWdNQ"
      },
      "source": [
        "# https://github.com/shawn-terryah/Twitter_Geolocation#step-4-train-a-meta-level-random-forest-classifier\n",
        "# https://stackabuse.com/seaborn-library-for-data-visualization-in-python-part-1/\n",
        "# https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd\n",
        "\n",
        "!unzip /content/no_meta_df.zip\n",
        "!unzip /content/yes_meta_df.zip\n",
        "!unzip /content/control_meta_df.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-KYx8Sg1xYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9cdfdf-3188-4fd3-a256-36eb9c17c53d"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnCEMWgKPTtQ"
      },
      "source": [
        "# nan_value = float(\"NaN\")\n",
        "# yes_df.replace(\"\", nan_value, inplace=True)\n",
        "# yes_df.dropna(subset = [\"full_text\"], inplace=True)\n",
        "# # yes_df = yes_df.sample(frac=0.1, replace=True, random_state=1)\n",
        "# print('new yes df shape: ', yes_df.shape)\n",
        "\n",
        "# no_df.replace(\"\", nan_value, inplace=True)\n",
        "# no_df.dropna(subset = [\"full_text\"], inplace=True)\n",
        "\n",
        "# no_df = no_df.sample(n=77447)\n",
        "# # no_df = no_df.sample(frac=0.1, replace=True, random_state=1)\n",
        "# print('new no df shape: ', no_df.shape)\n",
        "\n",
        "# control_df = control_df.sample(n=77447)\n",
        "# control_df.replace(\"\", nan_value, inplace=True)\n",
        "# control_df.dropna(subset = [\"full_text\"], inplace=True)\n",
        "\n",
        "# df = yes_df.append(control_df)\n",
        "# df['clean_text'] = df['full_text'].apply(lambda x: preprocess_text(x))\n",
        "# print('main df shape', df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cZ1InihgE0F"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import ast \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from nltk.tag.sequential import ClassifierBasedPOSTagger \n",
        "from nltk.corpus import treebank \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import demoji\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "demoji.download_codes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njUgA7KSaaCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6743054e-83ea-40e5-a079-b00f9031f57d"
      },
      "source": [
        "control_df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/control_meta_df4.pkl')\n",
        "no_df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/no_meta_df4.pkl')\n",
        "yes_df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/yes_meta_df4.pkl')\n",
        "\n",
        "print('control df: ', control_df.shape, '\\nyes df: ', yes_df.shape, '\\nno df: ', no_df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "control df:  (190158, 15) \n",
            "yes df:  (77446, 15) \n",
            "no df:  (95773, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vET2yvG0jiY"
      },
      "source": [
        "def preprocess_text(tweet):\n",
        "    tweet = str(tweet)\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    tweet = re.sub(r'\\@\\w+|\\#\\w+', '', tweet)\n",
        "    # tweet = re.sub(r'[@#]', '', tweet)\n",
        "    # demoji.download_codes()\n",
        "    tweet = demoji.replace(tweet, '')  # remove all emojis\n",
        "    # tweet = emoji.demojize(tweet)\n",
        "    # tweet = tweet.replace(\":\", \" \")\n",
        "    # tweet = tweet.replace(\"_\", \" \")\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
        "    # ps = PorterStemmer()\n",
        "    # stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "    return \" \".join(filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39uGcXL8Zfka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc0ef80-1c60-42c0-dfaa-62f7d0a8fc7d"
      },
      "source": [
        "control_df = control_df.sample(n=77446)\n",
        "no_df = no_df.sample(n=77446)\n",
        "df = yes_df.append(control_df)\n",
        "# df['clean_text'] = df['full_text'].apply(lambda x: preprocess_text(x))\n",
        "print('main df: ', df.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main df:  (154892, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUjX4g5k7f6N",
        "outputId": "fd654a70-ecf9-4896-b573-8154b741161b"
      },
      "source": [
        "df.label"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "1         1\n",
              "2         1\n",
              "3         1\n",
              "4         1\n",
              "         ..\n",
              "11173     0\n",
              "7637      0\n",
              "66481     0\n",
              "38589     0\n",
              "159286    0\n",
              "Name: label, Length: 154892, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pv_hdUz1L6G"
      },
      "source": [
        "# aggregation functions\n",
        "agg_funcs = {'full_text' : lambda x: ' '.join(x),\n",
        "             'statuses_count': np.max,\n",
        "             'friends_count' :np.mean,\n",
        "             'favourites_count' : np.mean,\n",
        "             'retweet_count' : np.mean,\n",
        "             'followers_count' : np.mean,\n",
        "             'label' : np.sum}\n",
        "\n",
        "# Groupby 'screen_name' and then apply the aggregation functions in agg_funcs\n",
        "df = df.fillna('')\n",
        "\n",
        "df = df.groupby(['screen_name']).agg(agg_funcs).reset_index()\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-IIB00v4zIx"
      },
      "source": [
        "df.label = df.label.apply(lambda x: 1 if x>0 else 0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4NYsLuL8VRU",
        "outputId": "4902b7ea-4ba7-4556-a215-966a9f861096"
      },
      "source": [
        "df.label"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      0\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "154    1\n",
              "155    0\n",
              "156    0\n",
              "157    0\n",
              "158    0\n",
              "Name: label, Length: 159, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFsQ2orhamW-"
      },
      "source": [
        "friends_count = df['friends_count'].values.reshape(df.shape[0], 1)\n",
        "statuses_count = df['statuses_count'].values.reshape(df.shape[0], 1)\n",
        "favourites_count = df['favourites_count'].values.reshape(df.shape[0], 1)\n",
        "followers_count = df['followers_count'].values.reshape(df.shape[0], 1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhztfM_cZLz"
      },
      "source": [
        "add_features = np.hstack((friends_count,statuses_count, favourites_count, followers_count))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QKwTo2-c2jR"
      },
      "source": [
        "# add_features.shape\n",
        "print(full_text_LR_decsfunc.shape, add_features.shape)\n",
        "X_train.shape\n",
        "\n",
        "# meta_X = np.hstack((full_text_LR_decsfunc, add_features))\n",
        "# meta_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeib71w5qU-B"
      },
      "source": [
        "meta_X = np.hstack((full_text_LR_decsfunc, add_features))\n",
        "این خط ارور دارد چون ابعاد متفاوت است.\n",
        "\n",
        "\n",
        "print(full_text_LR_decsfunc.shape, add_features.shape)\n",
        "X_train.shape\n",
        "\n",
        "(116169,) (154892, 4)\n",
        "\n",
        "(116169,)\n",
        "\n",
        "به همین دلیل است که در کد نمونه هم اول دیتاست را جدا کرده بعد بقیه مراحل. چون نتایج مدل لاجستیک رگرششن مربوط به ۷۰ درصد داده ها است که تصادفی انتخاب شده و ما نمیدانیم کدام داده ها تا متا دیتای مربوطه را به آنها بچسبانیم \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS1sIkWD49qM"
      },
      "source": [
        "df['clean_text'] = df['full_text'].apply(lambda x: preprocess_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2EafnwGnL6I"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(add_features, df['label'], random_state=0, shuffle=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ZK_1cBOvPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09434cc-d71e-42d2-9ae6-971860810e09"
      },
      "source": [
        "# X_train = X_train.fillna(X_train.mean())\n",
        "\n",
        "# vect = TfidfVectorizer(min_df=5).fit(X_train)  ## when using clean_text\n",
        "# X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "# vect = TfidfVectorizer(min_df=5).fit(X_train.values.astype('str'))  ## when using full_text\n",
        "# X_train_vectorized = vect.transform(X_train.values.astype('str'))\n",
        "\n",
        "model = LogisticRegression(max_iter=800)\n",
        "# model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# full_text_LR_decsfunc = model.decision_function(X_train_vectorized)  # for future use in RF\n",
        "\n",
        "# predictions = model.predict(vect.transform(X_test))\n",
        "\n",
        "# model = RandomForestClassifier(n_estimators=20, n_jobs=-1, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "y_pred = predictions\n",
        "print('AUC of RF: ', roc_auc_score(y_test, predictions))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC of RF:  0.7071611253196931\n",
            "Mean Absolute Error: 0.275\n",
            "Mean Squared Error: 0.275\n",
            "Root Mean Squared Error: 0.5244044240850758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMITxwfThCEe"
      },
      "source": [
        "count vectorizer AUC of LR model:  0.8000839890550887 for yes/control on full_text\n",
        "\n",
        "\n",
        "count vectorizer AUC of LR model:  0.7699083776501068 for yes/control on clean_text without emoji and on filtered words\n",
        "\n",
        "\n",
        "\n",
        "count vectorizer AUC of LR model:  0.7088813242976063 for yes/no on full_text\n",
        "\n",
        "\n",
        "count vectorizer AUC of LR model:  0.6822676596633498 for yes/no on clean_text without emoji and on filtered words\n",
        "\n",
        "# random forest on 20 estimator\n",
        "on yes/controm with unclean text:\n",
        "\n",
        "\n",
        "Mean Absolute Error: 0.23063812204632905\n",
        "\n",
        "Mean Squared Error: 0.23063812204632905\n",
        "\n",
        "Root Mean Squared Error: 0.48024797974205896\n",
        "\n",
        "AUC of RF:  0.7698903756887873\n",
        "\n",
        "# on 50 estimator\n",
        "\n",
        "AUC of RF:  0.7778957559166833\n",
        "\n",
        "Mean Absolute Error: 0.22289078842031868\n",
        "\n",
        "Mean Squared Error: 0.22289078842031868\n",
        "\n",
        "Root Mean Squared Error: 0.47211310977383236\n",
        "\n",
        "\n",
        "# on 100 estimator\n",
        "\n",
        "AUC of RF:  0.7831501854664477\n",
        "\n",
        "Mean Absolute Error: 0.21772589933631176\n",
        "\n",
        "Mean Squared Error: 0.21772589933631176\n",
        "\n",
        "Root Mean Squared Error: 0.46661107931157375\n",
        "\n",
        "# on 500 esimator\n",
        "\n",
        "AUC of RF:  0.785947674515556\n",
        "\n",
        "Mean Absolute Error: 0.21501433256720812\n",
        "\n",
        "Mean Squared Error: 0.21501433256720812\n",
        "\n",
        "Root Mean Squared Error: 0.4636963797219126\n",
        "\n",
        "\n",
        "# with adding meta features \n",
        "\n",
        "friends_count,statuses_count, favourites_count,followers_count\n",
        "\n",
        "UC of RF:  1.0\n",
        "Mean Absolute Error: 0.0\n",
        "Mean Squared Error: 0.0\n",
        "Root Mean Squared Error: 0.0\n",
        "\n",
        "\n",
        "# with meta features and group bying df on screen_name:\n",
        "\n",
        "UC of RF:  0.7071611253196931\n",
        "\n",
        "Mean Absolute Error: 0.275\n",
        "\n",
        "Mean Squared Error: 0.275\n",
        "\n",
        "Root Mean Squared Error: 0.5244044240850758\n",
        "\n",
        "\n",
        "# AUC of LR on yes/control full_text\n",
        "\n",
        "UC of RF:  0.8018956491925989\n",
        "\n",
        "Mean Absolute Error: 0.19879658084342638\n",
        "\n",
        "Mean Squared Error: 0.19879658084342638\n",
        "\n",
        "Root Mean Squared Error: 0.4458661019223444\n",
        "\n",
        "# AUC of LR on no/control full_twxt. so what does it mean?\n",
        "\n",
        "UC of RF:  0.7843411862248555\n",
        "Mean Absolute Error: 0.21491103478552798\n",
        "Mean Squared Error: 0.21491103478552798\n",
        "Root Mean Squared Error: 0.46358498119064206\n",
        "\n",
        "\n",
        "# AUC of LR on control/control half and half labeled to 0/1 randomly \n",
        "# yes/yes and no/no daase seperatly produced same AUC\n",
        "\n",
        "UC of RF:  0.49830601467019386\n",
        "\n",
        "Mean Absolute Error: 0.5016010742691871\n",
        "\n",
        "Mean Squared Error: 0.5016010742691871\n",
        "\n",
        "Root Mean Squared Error: 0.7082380067951642\n",
        "\n"
      ]
    }
  ]
}